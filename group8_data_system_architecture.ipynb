{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yadhu200609204/YadhuKrishnaRam.Ponnusamy.BDAT1004PS1/blob/main/group8_data_system_architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qJPW5fGq_iq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b259c31c-ab95-4a0a-c8ce-c3c0e75f82f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.24\" 2024-07-16\n",
            "OpenJDK Runtime Environment (build 11.0.24+8-post-Ubuntu-1ubuntu322.04)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.24+8-post-Ubuntu-1ubuntu322.04, mixed mode, sharing)\n"
          ]
        }
      ],
      "source": [
        "#CURRENT JAVA VERSION\n",
        "!java -version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOrVzuairQT-"
      },
      "outputs": [],
      "source": [
        "#INSTALLING JAVA 8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPNFR2Vwshc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23f1e319-2506-42da-bc1e-104a681777bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 choices for the alternative java (providing /usr/bin/java).\n",
            "\n",
            "  Selection    Path                                            Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/lib/jvm/java-11-openjdk-amd64/bin/java      1111      auto mode\n",
            "  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/java      1111      manual mode\n",
            "  2            /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java   1081      manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 2\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java to provide /usr/bin/java (java) in manual mode\n"
          ]
        }
      ],
      "source": [
        "#SWITCHING JAVA VERSION TO USE AS DEFAULT\n",
        "!update-alternatives --config java"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjdE9wLBsrvU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54f052c1-45b8-4511-dbc3-450a0c2ffbd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 choices for the alternative javac (providing /usr/bin/javac).\n",
            "\n",
            "  Selection    Path                                          Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/lib/jvm/java-11-openjdk-amd64/bin/javac   1111      auto mode\n",
            "  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/javac   1111      manual mode\n",
            "  2            /usr/lib/jvm/java-8-openjdk-amd64/bin/javac    1081      manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 2\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javac to provide /usr/bin/javac (javac) in manual mode\n"
          ]
        }
      ],
      "source": [
        "#SWITCHING JAVAC VERSION TO USE AS DEFAULT\n",
        "!update-alternatives --config javac"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOgYVN1ds2dy",
        "outputId": "0a70910f-d6b6-4791-a171-f5ec223ad232"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 choices for the alternative jps (providing /usr/bin/jps).\n",
            "\n",
            "  Selection    Path                                        Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/lib/jvm/java-11-openjdk-amd64/bin/jps   1111      auto mode\n",
            "  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/jps   1111      manual mode\n",
            "  2            /usr/lib/jvm/java-8-openjdk-amd64/bin/jps    1081      manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 2\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jps to provide /usr/bin/jps (jps) in manual mode\n"
          ]
        }
      ],
      "source": [
        "##SWITCHING JPS VERSION TO USE AS DEFAULT\n",
        "!update-alternatives --config jps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qKKhdvgs_TG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d859ecb4-78a6-45ad-e14b-41bddc3c2e52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"1.8.0_422\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_422-8u422-b05-1~22.04-b05)\n",
            "OpenJDK 64-Bit Server VM (build 25.422-b05, mixed mode)\n"
          ]
        }
      ],
      "source": [
        "#CHECKING JAVA VERSION\n",
        "!java -version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyvjADLcvMCP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78f63b6d-9362-4c1c-d2f2-3608b6231961"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/jvm/java-8-openjdk-amd64/jre/\n"
          ]
        }
      ],
      "source": [
        "#FINDING DEFAULT JAVA PATH\n",
        "!readlink -f /usr/bin/java | sed \"s:bin/java::\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bm6xlSUbvZ5j"
      },
      "outputs": [],
      "source": [
        "#Importing os module\n",
        "import os\n",
        "#Creating environment variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"JRE_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64/jre\"\n",
        "os.environ[\"PATH\"] += \":$JAVA_HOME/bin:$JRE_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLc3H3J1wiDh"
      },
      "outputs": [],
      "source": [
        "#purging before installation\n",
        "!apt-get purge openssh-server -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W35qbcdIw3CU"
      },
      "outputs": [],
      "source": [
        "#Installing openssh-server\n",
        "!apt-get install openssh-server -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoeMNv6qxCDg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "509a3758-9a27-492f-df6a-f66032f01c9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Starting OpenBSD Secure Shell server sshd\n",
            "   ...done.\n"
          ]
        }
      ],
      "source": [
        "#Starting the server\n",
        "!service ssh start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mg3-wEwnxgmc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c26295e9-022f-42d3-a077-c91bfa006037"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Port 22\n",
            "#GatewayPorts no\n"
          ]
        }
      ],
      "source": [
        "!grep Port /etc/ssh/sshd_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHXdY4_BxoOG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eb26358-e984-49c0-a965-850192084013"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating public/private rsa key pair.\n",
            "Created directory '/root/.ssh'.\n",
            "Your identification has been saved in /root/.ssh/id_rsa\n",
            "Your public key has been saved in /root/.ssh/id_rsa.pub\n",
            "The key fingerprint is:\n",
            "SHA256:QW2pk79QIGC1AODa2iUV88vqKJbj5waaMs39bPmCKlo root@c6d305e3d43e\n",
            "The key's randomart image is:\n",
            "+---[RSA 3072]----+\n",
            "|....*o. .. .     |\n",
            "|.  . =.o. +      |\n",
            "| .  . o..=       |\n",
            "|.. . . .+..      |\n",
            "|. o . o S+       |\n",
            "| + o .  . .      |\n",
            "|o+E... . . .     |\n",
            "|B=o=o.+   .      |\n",
            "|*=Bo.ooo.        |\n",
            "+----[SHA256]-----+\n"
          ]
        }
      ],
      "source": [
        "#CREATION OF NEW RSA KEY PAIR\n",
        "!ssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7QMgmizxzz5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d0a92f8-c398-4a91-903c-90bdf2ab62f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCcNJes2o59ttObr/Qy7AxQ0vY2kLkoe7a/K90nVS5eVls5i+vVR8KoJQDmxUnv\n",
            "aXLlo48Y0skLrp4mx1cnUhKzqZNp63HNAG6SFNJVkxBwOMBY7GbHLg0CJL5NPblZojMPXxFkgKiNVpYMjstzmlaG4wMjUIxiB3H3\n",
            "xQ79gSfa144BofLjpPS6vtkMWxClXDUuXiZIiba+XP8WtZoChjl31MHcFzCr5Wp5KG6FH/8g5NGLlAiiUZfhxZ/AYbQ/wGFikSes\n",
            "Gu0LAAbMmT13X2TIfmH/mDsDj53WMA/Kr6ba7e8CBeCQT7T4UAGMBFuGtsr+zBU29NvgPwZb2Zfk+BFoBE9RnSrOJM9VYC+U+ynR\n",
            "VNZm37n2nsslVhmq8/wNSPQ7/KHrNsiNQck/bjdnz25ZMeSp5JIhsvB7CznTWIbYm1JjO40ZiiWIBCI7wZ+io3WjLzMg1l3rmKGz\n",
            "T/JpGlSgr8Uk9qcEMRaCrk4USdNqyt9snOIuAR1OTuKsdpw3gPc= root@c6d305e3d43e\n"
          ]
        }
      ],
      "source": [
        "#PUBLIC KEY\n",
        "!more /root/.ssh/id_rsa.pub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xomkeP7-x2Z7"
      },
      "outputs": [],
      "source": [
        "#COPYING KEY\n",
        "!cat $HOME/.ssh/id_rsa.pub>>$HOME/.ssh/authorized_keys\n",
        "#Changing the permissions on the key\n",
        "!chmod 0600 ~/.ssh/authorized_keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1HHC5PGx6pe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9105fecb-daee-42ce-f60d-3dcc2b5acf1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.\r\n",
            " 07:26:50 up 3 min,  0 users,  load average: 1.88, 0.84, 0.33\n"
          ]
        }
      ],
      "source": [
        "#CONNECTING WITH LOCAL MACHINE\n",
        "!ssh -o StrictHostKeyChecking=no localhost uptime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31xXhlFzyKG5"
      },
      "outputs": [],
      "source": [
        "#Downloading Hadoop 3.2.3\n",
        "!wget -q https://archive.apache.org/dist/hadoop/common/hadoop-3.2.3/hadoop-3.2.3.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZzqCBHpyLjs"
      },
      "outputs": [],
      "source": [
        "#Extracting the contents of the archive\n",
        "!sudo tar -xzf hadoop-3.2.3.tar.gz\n",
        "#Removing the tar file\n",
        "!rm hadoop-3.2.3.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uN6wJb5yaMU"
      },
      "outputs": [],
      "source": [
        "#Copying the hadoop files to user/local\n",
        "!cp -r hadoop-3.2.3/ /usr/local/\n",
        "#-r copy directories recursively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8JrAyBj0PiH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d739048-ef03-4665-f54b-3eef769c64f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "capacity-scheduler.xml\t\t  httpfs-log4j.properties     mapred-site.xml\n",
            "configuration.xsl\t\t  httpfs-signature.secret     shellprofile.d\n",
            "container-executor.cfg\t\t  httpfs-site.xml\t      ssl-client.xml.example\n",
            "core-site.xml\t\t\t  kms-acls.xml\t\t      ssl-server.xml.example\n",
            "hadoop-env.cmd\t\t\t  kms-env.sh\t\t      user_ec_policies.xml.template\n",
            "hadoop-env.sh\t\t\t  kms-log4j.properties\t      workers\n",
            "hadoop-metrics2.properties\t  kms-site.xml\t\t      yarn-env.cmd\n",
            "hadoop-policy.xml\t\t  log4j.properties\t      yarn-env.sh\n",
            "hadoop-user-functions.sh.example  mapred-env.cmd\t      yarnservice-log4j.properties\n",
            "hdfs-site.xml\t\t\t  mapred-env.sh\t\t      yarn-site.xml\n",
            "httpfs-env.sh\t\t\t  mapred-queues.xml.template\n"
          ]
        }
      ],
      "source": [
        "#Exploring hadoop-3.2.3/etc/hadoop directory\n",
        "!ls /usr/local/hadoop-3.2.3/etc/hadoop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MB2aNRu50kpx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58422363-01a7-4ec5-8659-1031997b08d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#\n",
            "# Licensed to the Apache Software Foundation (ASF) under one\n",
            "# or more contributor license agreements.  See the NOTICE file\n",
            "# distributed with this work for additional information\n",
            "# regarding copyright ownership.  The ASF licenses this file\n",
            "# to you under the Apache License, Version 2.0 (the\n",
            "# \"License\"); you may not use this file except in compliance\n",
            "# with the License.  You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "\n",
            "# Set Hadoop-specific environment variables here.\n",
            "\n",
            "##\n",
            "## THIS FILE ACTS AS THE MASTER FILE FOR ALL HADOOP PROJECTS.\n",
            "## SETTINGS HERE WILL BE READ BY ALL HADOOP COMMANDS.  THEREFORE,\n",
            "## ONE CAN USE THIS FILE TO SET YARN, HDFS, AND MAPREDUCE\n",
            "## CONFIGURATION OPTIONS INSTEAD OF xxx-env.sh.\n",
            "##\n",
            "## Precedence rules:\n",
            "##\n",
            "## {yarn-env.sh|hdfs-env.sh} > hadoop-env.sh > hard-coded defaults\n",
            "##\n",
            "## {YARN_xyz|HDFS_xyz} > HADOOP_xyz > hard-coded defaults\n",
            "##\n",
            "\n",
            "# Many of the options here are built from the perspective that users\n",
            "# may want to provide OVERWRITING values on the command line.\n",
            "# For example:\n",
            "#\n",
            "#  JAVA_HOME=/usr/java/testing hdfs dfs -ls\n",
            "#\n",
            "# Therefore, the vast majority (BUT NOT ALL!) of these defaults\n",
            "# are configured for substitution and not append.  If append\n",
            "# is preferable, modify this file accordingly.\n",
            "\n",
            "###\n",
            "# Generic settings for HADOOP\n",
            "###\n",
            "\n",
            "# Technically, the only required environment variable is JAVA_HOME.\n",
            "# All others are optional.  However, the defaults are probably not\n",
            "# preferred.  Many sites configure these options outside of Hadoop,\n",
            "# such as in /etc/profile.d\n",
            "\n",
            "# The java implementation to use. By default, this environment\n",
            "# variable is REQUIRED on ALL platforms except OS X!\n",
            "# export JAVA_HOME=\n",
            "\n",
            "# Location of Hadoop.  By default, Hadoop will attempt to determine\n",
            "# this location based upon its execution path.\n",
            "# export HADOOP_HOME=\n",
            "\n",
            "# Location of Hadoop's configuration information.  i.e., where this\n",
            "# file is living. If this is not defined, Hadoop will attempt to\n",
            "# locate it based upon its execution path.\n",
            "#\n",
            "# NOTE: It is recommend that this variable not be set here but in\n",
            "# /etc/profile.d or equivalent.  Some options (such as\n",
            "# --config) may react strangely otherwise.\n",
            "#\n",
            "# export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop\n",
            "\n",
            "# The maximum amount of heap to use (Java -Xmx).  If no unit\n",
            "# is provided, it will be converted to MB.  Daemons will\n",
            "# prefer any Xmx setting in their respective _OPT variable.\n",
            "# There is no default; the JVM will autoscale based upon machine\n",
            "# memory size.\n",
            "# export HADOOP_HEAPSIZE_MAX=\n",
            "\n",
            "# The minimum amount of heap to use (Java -Xms).  If no unit\n",
            "# is provided, it will be converted to MB.  Daemons will\n",
            "# prefer any Xms setting in their respective _OPT variable.\n",
            "# There is no default; the JVM will autoscale based upon machine\n",
            "# memory size.\n",
            "# export HADOOP_HEAPSIZE_MIN=\n",
            "\n",
            "# Enable extra debugging of Hadoop's JAAS binding, used to set up\n",
            "# Kerberos security.\n",
            "# export HADOOP_JAAS_DEBUG=true\n",
            "\n",
            "# Extra Java runtime options for all Hadoop commands. We don't support\n",
            "# IPv6 yet/still, so by default the preference is set to IPv4.\n",
            "# export HADOOP_OPTS=\"-Djava.net.preferIPv4Stack=true\"\n",
            "# For Kerberos debugging, an extended option set logs more information\n",
            "# export HADOOP_OPTS=\"-Djava.net.preferIPv4Stack=true -Dsun.security.krb5.debug=true -Dsun.security.spnego.debug\"\n",
            "\n",
            "# Some parts of the shell code may do special things dependent upon\n",
            "# the operating system.  We have to set this here. See the next\n",
            "# section as to why....\n",
            "export HADOOP_OS_TYPE=${HADOOP_OS_TYPE:-$(uname -s)}\n",
            "\n",
            "# Extra Java runtime options for some Hadoop commands\n",
            "# and clients (i.e., hdfs dfs -blah).  These get appended to HADOOP_OPTS for\n",
            "# such commands.  In most cases, # this should be left empty and\n",
            "# let users supply it on the command line.\n",
            "# export HADOOP_CLIENT_OPTS=\"\"\n",
            "\n",
            "#\n",
            "# A note about classpaths.\n",
            "#\n",
            "# By default, Apache Hadoop overrides Java's CLASSPATH\n",
            "# environment variable.  It is configured such\n",
            "# that it starts out blank with new entries added after passing\n",
            "# a series of checks (file/dir exists, not already listed aka\n",
            "# de-deduplication).  During de-deduplication, wildcards and/or\n",
            "# directories are *NOT* expanded to keep it simple. Therefore,\n",
            "# if the computed classpath has two specific mentions of\n",
            "# awesome-methods-1.0.jar, only the first one added will be seen.\n",
            "# If two directories are in the classpath that both contain\n",
            "# awesome-methods-1.0.jar, then Java will pick up both versions.\n",
            "\n",
            "# An additional, custom CLASSPATH. Site-wide configs should be\n",
            "# handled via the shellprofile functionality, utilizing the\n",
            "# hadoop_add_classpath function for greater control and much\n",
            "# harder for apps/end-users to accidentally override.\n",
            "# Similarly, end users should utilize ${HOME}/.hadooprc .\n",
            "# This variable should ideally only be used as a short-cut,\n",
            "# interactive way for temporary additions on the command line.\n",
            "# export HADOOP_CLASSPATH=\"/some/cool/path/on/your/machine\"\n",
            "\n",
            "# Should HADOOP_CLASSPATH be first in the official CLASSPATH?\n",
            "# export HADOOP_USER_CLASSPATH_FIRST=\"yes\"\n",
            "\n",
            "# If HADOOP_USE_CLIENT_CLASSLOADER is set, the classpath along\n",
            "# with the main jar are handled by a separate isolated\n",
            "# client classloader when 'hadoop jar', 'yarn jar', or 'mapred job'\n",
            "# is utilized. If it is set, HADOOP_CLASSPATH and\n",
            "# HADOOP_USER_CLASSPATH_FIRST are ignored.\n",
            "# export HADOOP_USE_CLIENT_CLASSLOADER=true\n",
            "\n",
            "# HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES overrides the default definition of\n",
            "# system classes for the client classloader when HADOOP_USE_CLIENT_CLASSLOADER\n",
            "# is enabled. Names ending in '.' (period) are treated as package names, and\n",
            "# names starting with a '-' are treated as negative matches. For example,\n",
            "# export HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES=\"-org.apache.hadoop.UserClass,java.,javax.,org.apache.hadoop.\"\n",
            "\n",
            "# Enable optional, bundled Hadoop features\n",
            "# This is a comma delimited list.  It may NOT be overridden via .hadooprc\n",
            "# Entries may be added/removed as needed.\n",
            "# export HADOOP_OPTIONAL_TOOLS=\"hadoop-kafka,hadoop-openstack,hadoop-azure-datalake,hadoop-aliyun,hadoop-aws,hadoop-azure\"\n",
            "\n",
            "###\n",
            "# Options for remote shell connectivity\n",
            "###\n",
            "\n",
            "# There are some optional components of hadoop that allow for\n",
            "# command and control of remote hosts.  For example,\n",
            "# start-dfs.sh will attempt to bring up all NNs, DNS, etc.\n",
            "\n",
            "# Options to pass to SSH when one of the \"log into a host and\n",
            "# start/stop daemons\" scripts is executed\n",
            "# export HADOOP_SSH_OPTS=\"-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10s\"\n",
            "\n",
            "# The built-in ssh handler will limit itself to 10 simultaneous connections.\n",
            "# For pdsh users, this sets the fanout size ( -f )\n",
            "# Change this to increase/decrease as necessary.\n",
            "# export HADOOP_SSH_PARALLEL=10\n",
            "\n",
            "# Filename which contains all of the hosts for any remote execution\n",
            "# helper scripts # such as workers.sh, start-dfs.sh, etc.\n",
            "# export HADOOP_WORKERS=\"${HADOOP_CONF_DIR}/workers\"\n",
            "\n",
            "###\n",
            "# Options for all daemons\n",
            "###\n",
            "#\n",
            "\n",
            "#\n",
            "# Many options may also be specified as Java properties.  It is\n",
            "# very common, and in many cases, desirable, to hard-set these\n",
            "# in daemon _OPTS variables.  Where applicable, the appropriate\n",
            "# Java property is also identified.  Note that many are re-used\n",
            "# or set differently in certain contexts (e.g., secure vs\n",
            "# non-secure)\n",
            "#\n",
            "\n",
            "# Where (primarily) daemon log files are stored.\n",
            "# ${HADOOP_HOME}/logs by default.\n",
            "# Java property: hadoop.log.dir\n",
            "# export HADOOP_LOG_DIR=${HADOOP_HOME}/logs\n",
            "\n",
            "# A string representing this instance of hadoop. $USER by default.\n",
            "# This is used in writing log and pid files, so keep that in mind!\n",
            "# Java property: hadoop.id.str\n",
            "# export HADOOP_IDENT_STRING=$USER\n",
            "\n",
            "# How many seconds to pause after stopping a daemon\n",
            "# export HADOOP_STOP_TIMEOUT=5\n",
            "\n",
            "# Where pid files are stored.  /tmp by default.\n",
            "# export HADOOP_PID_DIR=/tmp\n",
            "\n",
            "# Default log4j setting for interactive commands\n",
            "# Java property: hadoop.root.logger\n",
            "# export HADOOP_ROOT_LOGGER=INFO,console\n",
            "\n",
            "# Default log4j setting for daemons spawned explicitly by\n",
            "# --daemon option of hadoop, hdfs, mapred and yarn command.\n",
            "# Java property: hadoop.root.logger\n",
            "# export HADOOP_DAEMON_ROOT_LOGGER=INFO,RFA\n",
            "\n",
            "# Default log level and output location for security-related messages.\n",
            "# You will almost certainly want to change this on a per-daemon basis via\n",
            "# the Java property (i.e., -Dhadoop.security.logger=foo). (Note that the\n",
            "# defaults for the NN and 2NN override this by default.)\n",
            "# Java property: hadoop.security.logger\n",
            "# export HADOOP_SECURITY_LOGGER=INFO,NullAppender\n",
            "\n",
            "# Default process priority level\n",
            "# Note that sub-processes will also run at this level!\n",
            "# export HADOOP_NICENESS=0\n",
            "\n",
            "# Default name for the service level authorization file\n",
            "# Java property: hadoop.policy.file\n",
            "# export HADOOP_POLICYFILE=\"hadoop-policy.xml\"\n",
            "\n",
            "#\n",
            "# NOTE: this is not used by default!  <-----\n",
            "# You can define variables right here and then re-use them later on.\n",
            "# For example, it is common to use the same garbage collection settings\n",
            "# for all the daemons.  So one could define:\n",
            "#\n",
            "# export HADOOP_GC_SETTINGS=\"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps\"\n",
            "#\n",
            "# .. and then use it as per the b option under the namenode.\n",
            "\n",
            "###\n",
            "# Secure/privileged execution\n",
            "###\n",
            "\n",
            "#\n",
            "# Out of the box, Hadoop uses jsvc from Apache Commons to launch daemons\n",
            "# on privileged ports.  This functionality can be replaced by providing\n",
            "# custom functions.  See hadoop-functions.sh for more information.\n",
            "#\n",
            "\n",
            "# The jsvc implementation to use. Jsvc is required to run secure datanodes\n",
            "# that bind to privileged ports to provide authentication of data transfer\n",
            "# protocol.  Jsvc is not required if SASL is configured for authentication of\n",
            "# data transfer protocol using non-privileged ports.\n",
            "# export JSVC_HOME=/usr/bin\n",
            "\n",
            "#\n",
            "# This directory contains pids for secure and privileged processes.\n",
            "#export HADOOP_SECURE_PID_DIR=${HADOOP_PID_DIR}\n",
            "\n",
            "#\n",
            "# This directory contains the logs for secure and privileged processes.\n",
            "# Java property: hadoop.log.dir\n",
            "# export HADOOP_SECURE_LOG=${HADOOP_LOG_DIR}\n",
            "\n",
            "#\n",
            "# When running a secure daemon, the default value of HADOOP_IDENT_STRING\n",
            "# ends up being a bit bogus.  Therefore, by default, the code will\n",
            "# replace HADOOP_IDENT_STRING with HADOOP_xx_SECURE_USER.  If one wants\n",
            "# to keep HADOOP_IDENT_STRING untouched, then uncomment this line.\n",
            "# export HADOOP_SECURE_IDENT_PRESERVE=\"true\"\n",
            "\n",
            "###\n",
            "# NameNode specific parameters\n",
            "###\n",
            "\n",
            "# Default log level and output location for file system related change\n",
            "# messages. For non-namenode daemons, the Java property must be set in\n",
            "# the appropriate _OPTS if one wants something other than INFO,NullAppender\n",
            "# Java property: hdfs.audit.logger\n",
            "# export HDFS_AUDIT_LOGGER=INFO,NullAppender\n",
            "\n",
            "# Specify the JVM options to be used when starting the NameNode.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# a) Set JMX options\n",
            "# export HDFS_NAMENODE_OPTS=\"-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1026\"\n",
            "#\n",
            "# b) Set garbage collection logs\n",
            "# export HDFS_NAMENODE_OPTS=\"${HADOOP_GC_SETTINGS} -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')\"\n",
            "#\n",
            "# c) ... or set them directly\n",
            "# export HDFS_NAMENODE_OPTS=\"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')\"\n",
            "\n",
            "# this is the default:\n",
            "# export HDFS_NAMENODE_OPTS=\"-Dhadoop.security.logger=INFO,RFAS\"\n",
            "\n",
            "###\n",
            "# SecondaryNameNode specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the SecondaryNameNode.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# This is the default:\n",
            "# export HDFS_SECONDARYNAMENODE_OPTS=\"-Dhadoop.security.logger=INFO,RFAS\"\n",
            "\n",
            "###\n",
            "# DataNode specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the DataNode.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# This is the default:\n",
            "# export HDFS_DATANODE_OPTS=\"-Dhadoop.security.logger=ERROR,RFAS\"\n",
            "\n",
            "# On secure datanodes, user to run the datanode as after dropping privileges.\n",
            "# This **MUST** be uncommented to enable secure HDFS if using privileged ports\n",
            "# to provide authentication of data transfer protocol.  This **MUST NOT** be\n",
            "# defined if SASL is configured for authentication of data transfer protocol\n",
            "# using non-privileged ports.\n",
            "# This will replace the hadoop.id.str Java property in secure mode.\n",
            "# export HDFS_DATANODE_SECURE_USER=hdfs\n",
            "\n",
            "# Supplemental options for secure datanodes\n",
            "# By default, Hadoop uses jsvc which needs to know to launch a\n",
            "# server jvm.\n",
            "# export HDFS_DATANODE_SECURE_EXTRA_OPTS=\"-jvm server\"\n",
            "\n",
            "###\n",
            "# NFS3 Gateway specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the NFS3 Gateway.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_NFS3_OPTS=\"\"\n",
            "\n",
            "# Specify the JVM options to be used when starting the Hadoop portmapper.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_PORTMAP_OPTS=\"-Xmx512m\"\n",
            "\n",
            "# Supplemental options for priviliged gateways\n",
            "# By default, Hadoop uses jsvc which needs to know to launch a\n",
            "# server jvm.\n",
            "# export HDFS_NFS3_SECURE_EXTRA_OPTS=\"-jvm server\"\n",
            "\n",
            "# On privileged gateways, user to run the gateway as after dropping privileges\n",
            "# This will replace the hadoop.id.str Java property in secure mode.\n",
            "# export HDFS_NFS3_SECURE_USER=nfsserver\n",
            "\n",
            "###\n",
            "# ZKFailoverController specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the ZKFailoverController.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_ZKFC_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# QuorumJournalNode specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the QuorumJournalNode.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_JOURNALNODE_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# HDFS Balancer specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the HDFS Balancer.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_BALANCER_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# HDFS Mover specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the HDFS Mover.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_MOVER_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# Router-based HDFS Federation specific parameters\n",
            "# Specify the JVM options to be used when starting the RBF Routers.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_DFSROUTER_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# HDFS StorageContainerManager specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the HDFS Storage Container Manager.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_STORAGECONTAINERMANAGER_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# Advanced Users Only!\n",
            "###\n",
            "\n",
            "#\n",
            "# When building Hadoop, one can add the class paths to the commands\n",
            "# via this special env var:\n",
            "# export HADOOP_ENABLE_BUILD_PATHS=\"true\"\n",
            "\n",
            "#\n",
            "# To prevent accidents, shell commands be (superficially) locked\n",
            "# to only allow certain users to execute certain subcommands.\n",
            "# It uses the format of (command)_(subcommand)_USER.\n",
            "#\n",
            "# For example, to limit who can execute the namenode command,\n",
            "# export HDFS_NAMENODE_USER=hdfs\n"
          ]
        }
      ],
      "source": [
        "#Exploring hadoop-env.sh file\n",
        "!cat /usr/local/hadoop-3.2.3/etc/hadoop/hadoop-env.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODHSq_KIB09F"
      },
      "outputs": [],
      "source": [
        "#Adding JAVA_HOME directory to hadoop-env.sh file\n",
        "!sed -i '/export JAVA_HOME=/a export JAVA_HOME=\\/usr\\/lib\\/jvm\\/java-8-openjdk-amd64' /usr/local/hadoop-3.2.3/etc/hadoop/hadoop-env.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KX5d2f5DCLXq"
      },
      "outputs": [],
      "source": [
        "#Creating Hadoop home variable\n",
        "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop-3.2.3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fEmiKqECSjx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3128fbb-a67a-42d7-c7ed-e5f11c77893c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/hadoop-3.2.3/etc/hadoop/capacity-scheduler.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/core-site.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/hadoop-policy.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/hdfs-site.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/httpfs-site.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/kms-acls.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/kms-site.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/mapred-site.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/yarn-site.xml\n"
          ]
        }
      ],
      "source": [
        "#Exploring hadoop-3.2.3/etc/hadoop xml files\n",
        "!ls $HADOOP_HOME/etc/hadoop/*.xml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rj-F2xwRC-f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a32c462-c0ac-4fe9-ec8c-d1555f42a382"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
            "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
            "<!--\n",
            "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "  you may not use this file except in compliance with the License.\n",
            "  You may obtain a copy of the License at\n",
            "\n",
            "    http://www.apache.org/licenses/LICENSE-2.0\n",
            "\n",
            "  Unless required by applicable law or agreed to in writing, software\n",
            "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "  See the License for the specific language governing permissions and\n",
            "  limitations under the License. See accompanying LICENSE file.\n",
            "-->\n",
            "\n",
            "<!-- Put site-specific property overrides in this file. -->\n",
            "\n",
            "<configuration>\n",
            "</configuration>\n"
          ]
        }
      ],
      "source": [
        "#Content of core-site.xml file\n",
        "!cat $HADOOP_HOME/etc/hadoop/core-site.xml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34wcLKPlDIB8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "352680c2-5c93-412d-b7ba-f6ab6d872349"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.3.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.3.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.3.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.3.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.3.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.3.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.3-tests.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.3.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.3.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.3.jar\n",
            "/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar\n"
          ]
        }
      ],
      "source": [
        "#Exploring mapreduce tools\n",
        "!ls $HADOOP_HOME/share/hadoop/mapreduce/*.jar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_j0OX1MDfEV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "3ffb2885-cdb0-471f-d39d-6a13b8563cc9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e9337c48-8249-4be0-9704-ee288b8059ee\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e9337c48-8249-4be0-9704-ee288b8059ee\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving BDAT1002_A3_8_Paragraph.txt to BDAT1002_A3_8_Paragraph.txt\n"
          ]
        }
      ],
      "source": [
        "#UPLOADING THE FILE\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfgB6pvWIejx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c4c84cb-0d2a-47d7-81e8-bbbc465951c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-08 07:27:52,787 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-11-08 07:27:52,887 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-11-08 07:27:52,887 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-11-08 07:27:53,066 INFO input.FileInputFormat: Total input files to process : 1\n",
            "2024-11-08 07:27:53,105 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-11-08 07:27:53,286 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local994378406_0001\n",
            "2024-11-08 07:27:53,286 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-11-08 07:27:53,498 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-11-08 07:27:53,499 INFO mapreduce.Job: Running job: job_local994378406_0001\n",
            "2024-11-08 07:27:53,506 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-11-08 07:27:53,516 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-11-08 07:27:53,516 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-11-08 07:27:53,522 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "2024-11-08 07:27:53,582 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-11-08 07:27:53,583 INFO mapred.LocalJobRunner: Starting task: attempt_local994378406_0001_m_000000_0\n",
            "2024-11-08 07:27:53,612 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-11-08 07:27:53,612 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-11-08 07:27:53,650 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-11-08 07:27:53,657 INFO mapred.MapTask: Processing split: file:/content/BDAT1002_A3_8_Paragraph.txt:0+1662\n",
            "2024-11-08 07:27:53,757 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-11-08 07:27:53,758 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-11-08 07:27:53,758 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-11-08 07:27:53,758 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-11-08 07:27:53,758 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-11-08 07:27:53,763 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-11-08 07:27:53,776 INFO mapred.LocalJobRunner: \n",
            "2024-11-08 07:27:53,776 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-11-08 07:27:53,777 INFO mapred.MapTask: Spilling map output\n",
            "2024-11-08 07:27:53,777 INFO mapred.MapTask: bufstart = 0; bufend = 2677; bufvoid = 104857600\n",
            "2024-11-08 07:27:53,777 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213356(104853424); length = 1041/6553600\n",
            "2024-11-08 07:27:53,803 INFO mapred.MapTask: Finished spill 0\n",
            "2024-11-08 07:27:53,818 INFO mapred.Task: Task:attempt_local994378406_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-11-08 07:27:53,822 INFO mapred.LocalJobRunner: map\n",
            "2024-11-08 07:27:53,823 INFO mapred.Task: Task 'attempt_local994378406_0001_m_000000_0' done.\n",
            "2024-11-08 07:27:53,831 INFO mapred.Task: Final Counters for attempt_local994378406_0001_m_000000_0: Counters: 18\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=318290\n",
            "\t\tFILE: Number of bytes written=863065\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=261\n",
            "\t\tMap output bytes=2677\n",
            "\t\tMap output materialized bytes=2312\n",
            "\t\tInput split bytes=106\n",
            "\t\tCombine input records=261\n",
            "\t\tCombine output records=174\n",
            "\t\tSpilled Records=174\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=204996608\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1662\n",
            "2024-11-08 07:27:53,831 INFO mapred.LocalJobRunner: Finishing task: attempt_local994378406_0001_m_000000_0\n",
            "2024-11-08 07:27:53,832 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-11-08 07:27:53,836 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-11-08 07:27:53,836 INFO mapred.LocalJobRunner: Starting task: attempt_local994378406_0001_r_000000_0\n",
            "2024-11-08 07:27:53,845 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-11-08 07:27:53,845 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-11-08 07:27:53,845 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-11-08 07:27:53,849 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7cc4028\n",
            "2024-11-08 07:27:53,851 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-11-08 07:27:53,888 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2117966208, maxSingleShuffleLimit=529491552, mergeThreshold=1397857792, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-11-08 07:27:53,900 INFO reduce.EventFetcher: attempt_local994378406_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-11-08 07:27:53,928 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local994378406_0001_m_000000_0 decomp: 2308 len: 2312 to MEMORY\n",
            "2024-11-08 07:27:53,932 INFO reduce.InMemoryMapOutput: Read 2308 bytes from map-output for attempt_local994378406_0001_m_000000_0\n",
            "2024-11-08 07:27:53,934 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2308, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2308\n",
            "2024-11-08 07:27:53,935 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-11-08 07:27:53,936 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-11-08 07:27:53,936 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-11-08 07:27:53,944 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-11-08 07:27:53,944 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2304 bytes\n",
            "2024-11-08 07:27:53,947 INFO reduce.MergeManagerImpl: Merged 1 segments, 2308 bytes to disk to satisfy reduce memory limit\n",
            "2024-11-08 07:27:53,948 INFO reduce.MergeManagerImpl: Merging 1 files, 2312 bytes from disk\n",
            "2024-11-08 07:27:53,949 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-11-08 07:27:53,949 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-11-08 07:27:53,949 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2304 bytes\n",
            "2024-11-08 07:27:53,950 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-11-08 07:27:53,953 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-11-08 07:27:53,983 INFO mapred.Task: Task:attempt_local994378406_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-11-08 07:27:53,987 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-11-08 07:27:53,992 INFO mapred.Task: Task attempt_local994378406_0001_r_000000_0 is allowed to commit now\n",
            "2024-11-08 07:27:53,998 INFO output.FileOutputCommitter: Saved output of task 'attempt_local994378406_0001_r_000000_0' to file:/content/output\n",
            "2024-11-08 07:27:54,001 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2024-11-08 07:27:54,001 INFO mapred.Task: Task 'attempt_local994378406_0001_r_000000_0' done.\n",
            "2024-11-08 07:27:54,002 INFO mapred.Task: Final Counters for attempt_local994378406_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=322946\n",
            "\t\tFILE: Number of bytes written=867011\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=174\n",
            "\t\tReduce shuffle bytes=2312\n",
            "\t\tReduce input records=174\n",
            "\t\tReduce output records=174\n",
            "\t\tSpilled Records=174\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=12\n",
            "\t\tTotal committed heap usage (bytes)=204996608\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=1634\n",
            "2024-11-08 07:27:54,003 INFO mapred.LocalJobRunner: Finishing task: attempt_local994378406_0001_r_000000_0\n",
            "2024-11-08 07:27:54,003 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-11-08 07:27:54,505 INFO mapreduce.Job: Job job_local994378406_0001 running in uber mode : false\n",
            "2024-11-08 07:27:54,506 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-11-08 07:27:54,507 INFO mapreduce.Job: Job job_local994378406_0001 completed successfully\n",
            "2024-11-08 07:27:54,515 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=641236\n",
            "\t\tFILE: Number of bytes written=1730076\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=261\n",
            "\t\tMap output bytes=2677\n",
            "\t\tMap output materialized bytes=2312\n",
            "\t\tInput split bytes=106\n",
            "\t\tCombine input records=261\n",
            "\t\tCombine output records=174\n",
            "\t\tReduce input groups=174\n",
            "\t\tReduce shuffle bytes=2312\n",
            "\t\tReduce input records=174\n",
            "\t\tReduce output records=174\n",
            "\t\tSpilled Records=348\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=12\n",
            "\t\tTotal committed heap usage (bytes)=409993216\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1662\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=1634\n"
          ]
        }
      ],
      "source": [
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar wordcount /content/BDAT1002_A3_8_Paragraph.txt /content/output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQVDR1aDJj2L",
        "outputId": "f8e24cd7-f71b-42ab-fd24-ff3a87adecbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-r-00000  _SUCCESS\n"
          ]
        }
      ],
      "source": [
        "!ls /content/output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iwz752dJM3al"
      },
      "outputs": [],
      "source": [
        "!cat /content/BDAT1002_A3_8_Paragraph.txt | tr '[:upper:]' '[:lower:]' | tr -c '[:alnum:]' '[\\n*]' | grep -v \"^$\" | sort | uniq -c | sort -nr > /content/BDAT1002_A1_8_WordCount.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2_C_77zY-0G",
        "outputId": "8c06b1b7-226c-4e25-847e-62867029f23c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      9 and\n",
            "      8 we\n",
            "      7 the\n",
            "      7 it\n",
            "      6 a\n",
            "      5 in\n",
            "      4 work\n",
            "      4 to\n",
            "      4 human\n",
            "      4 data\n",
            "      4 are\n",
            "      4 also\n",
            "      3 was\n",
            "      3 them\n",
            "      3 new\n",
            "      3 is\n",
            "      3 for\n",
            "      3 computer\n",
            "      3 but\n",
            "      2 tool\n",
            "      2 their\n",
            "      2 that\n",
            "      2 technologies\n",
            "      2 significant\n",
            "      2 s\n",
            "      2 role\n",
            "      2 really\n",
            "      2 prevents\n",
            "      2 own\n",
            "      2 our\n",
            "      2 only\n",
            "      2 of\n",
            "      2 not\n",
            "      2 make\n",
            "      2 kumar\n",
            "      2 knowledge\n",
            "      2 industry\n",
            "      2 feel\n",
            "      2 concerned\n",
            "      2 big\n",
            "      2 as\n",
            "      2 any\n",
            "      2 ai\n",
            "      2 about\n",
            "      1 years\n",
            "      1 yadhu\n",
            "      1 without\n",
            "      1 with\n",
            "      1 will\n",
            "      1 why\n",
            "      1 which\n",
            "      1 where\n",
            "      1 when\n",
            "      1 way\n",
            "      1 used\n",
            "      1 use\n",
            "      1 us\n",
            "      1 up\n",
            "      1 undertake\n",
            "      1 udayan\n",
            "      1 udaya\n",
            "      1 trend\n",
            "      1 through\n",
            "      1 this\n",
            "      1 thirumurugan\n",
            "      1 think\n",
            "      1 they\n",
            "      1 technology\n",
            "      1 talent\n",
            "      1 surpass\n",
            "      1 study\n",
            "      1 strongly\n",
            "      1 starting\n",
            "      1 so\n",
            "      1 siva\n",
            "      1 similarly\n",
            "      1 show\n",
            "      1 same\n",
            "      1 robots\n",
            "      1 revolution\n",
            "      1 replacing\n",
            "      1 replacement\n",
            "      1 ram\n",
            "      1 question\n",
            "      1 ponnusamy\n",
            "      1 plays\n",
            "      1 plagiarized\n",
            "      1 paul\n",
            "      1 other\n",
            "      1 opportunities\n",
            "      1 opens\n",
            "      1 on\n",
            "      1 obviously\n",
            "      1 no\n",
            "      1 nevertheless\n",
            "      1 nandhakumar\n",
            "      1 moreover\n",
            "      1 more\n",
            "      1 members\n",
            "      1 matter\n",
            "      1 many\n",
            "      1 machine\n",
            "      1 learning\n",
            "      1 latest\n",
            "      1 kumaran\n",
            "      1 krishna\n",
            "      1 jayaprakash\n",
            "      1 its\n",
            "      1 invented\n",
            "      1 intelligent\n",
            "      1 integrity\n",
            "      1 innovations\n",
            "      1 inhibited\n",
            "      1 individual\n",
            "      1 humans\n",
            "      1 however\n",
            "      1 how\n",
            "      1 growth\n",
            "      1 group\n",
            "      1 gomez\n",
            "      1 gaining\n",
            "      1 future\n",
            "      1 from\n",
            "      1 finance\n",
            "      1 fascinated\n",
            "      1 every\n",
            "      1 essential\n",
            "      1 eradicating\n",
            "      1 engineering\n",
            "      1 driven\n",
            "      1 developments\n",
            "      1 development\n",
            "      1 developing\n",
            "      1 dependent\n",
            "      1 decisions\n",
            "      1 decision\n",
            "      1 debated\n",
            "      1 creativity\n",
            "      1 created\n",
            "      1 compromises\n",
            "      1 company\n",
            "      1 chose\n",
            "      1 careers\n",
            "      1 capability\n",
            "      1 cannot\n",
            "      1 can\n",
            "      1 broadens\n",
            "      1 bright\n",
            "      1 better\n",
            "      1 benefit\n",
            "      1 believe\n",
            "      1 beings\n",
            "      1 becomes\n",
            "      1 back\n",
            "      1 around\n",
            "      1 analyzing\n",
            "      1 an\n",
            "      1 among\n",
            "      1 advancing\n",
            "      1 academic\n",
            "      1 ability\n",
            "      1 200610016\n",
            "      1 200609204\n",
            "      1 200607772\n",
            "      1 200595039\n"
          ]
        }
      ],
      "source": [
        "!cat /content/BDAT1002_A1_8_WordCount.txt\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDhcxVkonvua0rzA076GUi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}